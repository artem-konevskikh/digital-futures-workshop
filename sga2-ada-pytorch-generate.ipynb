{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sga2-ada-pytorch-generate.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XH5qhxBngNpc"},"source":["# StyleGAN ADA PyTorch Generation\n","\n","by Artem Konevskikh\n","\n","based on dvschultz"]},{"cell_type":"code","metadata":{"cellView":"form","id":"T1X9H9xXgMdg"},"source":["#@title Mount Google Drive\n","#@markdown Mount Google Drive to load pretrained models and to save the results.\n","\n","#@markdown After running this cell you will get the link. Follow the link, grant access to your Drive and copy auth code.\n","\n","#@markdown Paste the code to the input below and press Enter\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7PeyWJfgjob","cellView":"form"},"source":["#@title Install\n","#@markdown StyleGAN2-ada will be installed to your Google Drive to speed up the training process\n","\n","#@markdown Run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary.\n","import os\n","import shlex\n","import numpy as np\n","\n","if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n","    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n","elif os.path.isdir(\"/content/drive/\"):\n","    #install script\n","    %cd \"/content/drive/MyDrive/\"\n","    !mkdir colab-sg2-ada-pytorch\n","    %cd colab-sg2-ada-pytorch\n","    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n","else:\n","    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    %cd pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n","    %cd ../\n","\n","!pip install ninja opensimplex\n","\n","%cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n","!git config --global user.name \"test\"\n","!git config --global user.email \"test@test.com\"\n","!git fetch origin\n","!git pull\n","!git stash\n","!git checkout origin/main -- train.py generate.py legacy.py closed_form_factorization.py flesh_digression.py apply_factor.py README.md calc_metrics.py training/stylegan2_multi.py training/training_loop.py util/utilgan.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFEHPByohDdG","cellView":"form"},"source":["#@title Convert Legacy Model\n","\n","#@markdown If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this workshop you do **not** need to use this cell.\n","\n","#@markdown Path to model that you want to convert \n","source_pkl = \"\" #@param {type: \"string\"}\n","#@markdown Path and file name to convert to.\n","dest_pkl = \"\" #@param {type: \"string\"}\n","\n","source_pkl = shlex.quote(source_pkl)\n","dest_pkl = shlex.quote(dest_pkl)\n","!python legacy.py --source={source_pkl} --dest={dest_pkl}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"el28DuPKmQ9P"},"source":["# Generation"]},{"cell_type":"code","metadata":{"id":"dNaj42CuwG06","cellView":"form"},"source":["#@title Generate Images\n","#@markdown Directory to save the generated images\n","outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/1\" #@param {type: \"string\"}\n","#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n","network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown **Generation Parameters**\n","\n","#@markdown Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n","truncation = 0.8 #@param {type: \"number\"}\n","#@markdown This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation. Provide one number or comma-separated list of integers\n","seeds = \"42,3333,80\" #@param {type: \"string\"}\n","#@markdown Generate random images (`seeds` parameter will be ignored)\n","gen_random = False #@param {type: \"boolean\"}\n","#@markdown Amount of random images to generate\n","n_imgs = 3 #@param {type: \"integer\"}\n","#@markdown ---\n","#@markdown **Generate Non-Square Images**\n","\n","#@markdown It's possible to make the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n","gen_nonsquare = False #@param {type: \"boolean\"}\n","#@markdown Width\n","width = 1920 #@param {type: \"integer\"}\n","#@markdown Height\n","height = 1080 #@param {type: \"integer\"}\n","#@markdown Padding style to apply in the additional space\n","scale_type = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n","\n","\n","if gen_random:\n","  seeds = ','.join(str(s) for s in list(set(list(np.random.randint(4294967295, size=n_imgs)))))\n","else:\n","  seeds = ','.join(str(s).strip() for s in seeds.split(','))\n","print(\"Seeds: \", seeds)\n","\n","nonsquare = f'--size={width}-{height} --scale-type={scale_type}' if gen_nonsquare else ''\n","\n","\n","outdir = shlex.quote(outdir)\n","network = shlex.quote(network)\n","!python generate.py --outdir={outdir} --trunc={truncation} --seeds={seeds} --network={network} {nonsquare}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEEr74W333U2","cellView":"form"},"source":["#@title Truncation Traversal\n","\n","#@markdown Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n","\n","#@markdown Directory to save the generated images\n","outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/2\" #@param {type: \"string\"}\n","#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n","network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n","\n","#@markdown Pass this only one seed. Pick a favorite from your generated images.\n","seed = 42  #@param {type: \"integer\"}\n","#@markdown Starting truncation value.\n","start = -1  #@param {type: \"number\"}\n","#@markdown Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n","stop = 1  #@param {type: \"number\"}\n","#@markdown How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)\n","increment = 0.01  #@param {type: \"number\"}\n","\n","outdir = shlex.quote(outdir)\n","network = shlex.quote(network)\n","!python generate.py --process=\"truncation\" --outdir={outdir} --start={start} --stop={stop} --increment={increment} --seeds={seed} --network={network}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGZVg1JxlPyZ","cellView":"form"},"source":["#@title Interpolation\n","#@markdown Directory to save the generated images\n","outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/3\" #@param {type: \"string\"}\n","#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n","network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown **Generation Parameters**\n","\n","#@markdown Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n","truncation = 0.8 #@param {type: \"number\"}\n","#@markdown This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation. Provide one number or comma-separated list of integers\n","seeds = \"42,333\" #@param {type: \"string\"}\n","#@markdown Generate random images (`seeds` parameter will be ignored)\n","gen_random = False #@param {type: \"boolean\"}\n","#@markdown Amount of random images to generate\n","n_imgs = 10 #@param {type: \"integer\"}\n","#@markdown Loop interpolation\n","gen_loop = True #@param {type: \"boolean\"}\n","#@markdown ---\n","#@markdown **Interpolation Parameters**\n","\n","#@markdown Interpolation type\n","interpolation = 'linear' #@param ['linear', 'slerp']\n","#@markdown Latent space\n","space = 'z' #@param ['z', 'w']\n","#@markdown ---\n","#@markdown **Generate Non-Square Images**\n","\n","#@markdown It's possible to make the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n","gen_nonsquare = False #@param {type: \"boolean\"}\n","#@markdown Width\n","width = 1920 #@param {type: \"integer\"}\n","#@markdown Height\n","height = 1080 #@param {type: \"integer\"}\n","#@markdown Padding style to apply in the additional space\n","scale_type = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n","\n","\n","if gen_random:\n","  seeds = list(set(list(np.random.randint(4294967295, size=n_imgs))))\n","  if gen_loop:\n","    seeds.append(seeds[-1])\n","  seeds = ','.join(str(s) for s in seeds)\n","else:\n","  seeds = ','.join(str(s).strip() for s in seeds.split(','))\n","print(\"Seeds: \", seeds)\n","\n","nonsquare = f'--size={width}-{height} --scale-type={scale_type}' if gen_nonsquare else ''\n","\n","outdir = shlex.quote(outdir)\n","network = shlex.quote(network)\n","!python generate.py --outdir={outdir} --trunc={truncation} --seeds={seeds} --network={network} {nonsquare} --space={space} --process=\"interpolation\" --interpolation={interpolation}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCbgH771qK_w","cellView":"form"},"source":["#@title Interpolation loops\n","#@markdown Directory to save the generated images\n","outdir = \"/content/drive/MyDrive/workshops/digitalfutures/results/4\" #@param {type: \"string\"}\n","#@markdown Path to the pretrained model. The most accurate way is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that here\n","network = \"/content/drive/MyDrive/workshops/digitalfutures/wood-clt-200.pkl\" #@param {type: \"string\"}\n","#@markdown ---\n","#@markdown **Generation Parameters**\n","\n","#@markdown Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite. \n","truncation = 0.8 #@param {type: \"number\"}\n","\n","#@markdown ---\n","#@markdown **Interpolation Parameters**\n","\n","#@markdown Interpolation type\n","interpolation = 'noiseloop' #@param ['noiseloop', 'circularloop']\n","#@markdown Number of frames\n","frames = 720 #@param {type: \"integer\"}\n","#@markdown This controls how \"wide\" the loop is. Make it smaller to show a less diverse range of samples. Make it larger to cover a lot of samples. This plus `frames` can help determine how fast the video feels.\n","diameter = 0.9 #@param {type: \"number\"}\n","#@markdown Starting place in the z space. Note: this value has nothing to do with the seeds you use to generate images. It just allows you to randomize your start point\n","random_seed = 42\n","\n","#@markdown ---\n","#@markdown **Generate Non-Square Images**\n","\n","#@markdown It's possible to make the model to output images that are not square. This isn’t as good as training a rectangular model, but with the right model it can still look nice.\n","gen_nonsquare = True #@param {type: \"boolean\"}\n","#@markdown Width\n","width = 1920 #@param {type: \"integer\"}\n","#@markdown Height\n","height = 1080 #@param {type: \"integer\"}\n","#@markdown Padding style to apply in the additional space\n","scale_type = 'pad' #@param ['pad', 'padside', 'symm', 'symmside']\n","\n","\n","nonsquare = f'--size={width}-{height} --scale-type={scale_type}' if gen_nonsquare else ''\n","\n","outdir = shlex.quote(outdir)\n","network = shlex.quote(network)\n","!python generate.py --outdir={outdir} --trunc={truncation} --network={network} {nonsquare} --process=\"interpolation\" --interpolation={interpolation} --diameter={diameter} --random_seed={random_seed} --frames={frames}"],"execution_count":null,"outputs":[]}]}